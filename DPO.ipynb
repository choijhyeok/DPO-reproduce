{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DPO 논문 실험 재현"
      ],
      "metadata": {
        "id": "7gE3lig-3NlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**실제 해당 코드 수행하실때는 A100 추천드립니다.**"
      ],
      "metadata": {
        "id": "a8hoTJaXL8di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- google mount시 에러 존재, google mount 안하는것 추천\n",
        "  - colab 상에서는 가끔 Transport endpoint is not connected 에러뜸\n",
        "  \n",
        "  - (코드 에러는 아니고 colab상에 다른 파일 많으면 발생 : [해당 에러 내용 참고](https://lv99.tistory.com/49))\n",
        "- v100 GPU, 대용량 ram 기준으로 실행, 그래프는 wandb"
      ],
      "metadata": {
        "id": "4y_BlyG-2ePE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/trl.git\n",
        "!pip install -q -U -r trl/examples/research_projects/stack_llama_2/scripts/requirements.txt\n",
        "!find trl/examples/research_projects/stack_llama_2/scripts -name \"sft_llama2.py\" -exec sed -i \"s/bf16/fp16/g\" {} \\;\n",
        "!accelerate config # This machine, No distributed training, NO, NO, NO, all, fp16\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "_LjDejctzbvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "P4bBF-8m_5am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실험 데이터셋"
      ],
      "metadata": {
        "id": "WojHzTKpC7ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TL;DR : https://aclanthology.org/W17-4508/\n",
        "- 같은 내용 hf : https://huggingface.co/datasets/webis/tldr-17 (webis/tldr-17) 이걸로 하면 DPO때 좀 이상해서 -> https://huggingface.co/datasets/banghua/tldr_reward_model_labeled 변경"
      ],
      "metadata": {
        "id": "QDDxW_IxDBJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "IHY8aU6LEhFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 논문 재현 실험 (논문에 나온 모델은 [github 참조](https://github.com/eric-mitchell/direct-preference-optimization/tree/main/config/model))"
      ],
      "metadata": {
        "id": "-pboR6jBEc_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPTJ"
      ],
      "metadata": {
        "id": "zaWUa7tREH3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 오류 수정내역\n",
        "\n",
        "/content/trl/examples/research_projects/stack_llama_2/scripts/sft_llama2.py 파일에서\n",
        "\n",
        "```\n",
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"Question: {example['question']}\\n\\nAnswer: {example['response_j']}\"\n",
        "    return text\n",
        "```\n",
        "\n",
        "부분 아래로 변경\n",
        "\n",
        "\n",
        "```\n",
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['chosen']}\"\n",
        "    return text\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "--------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
        "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
        "\n",
        "```\n",
        "\n",
        "위 부분 아래와 같이 변경 (model 저장시 gptj는 sage_serialization=True시 에러)\n",
        "\n",
        "```\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
        "model.save_pretrained(output_merged_dir)\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "nv4jkdG0Mxoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title llama2.py TL;DR 데이터 맞춤 (그냥 이부분 실행하면 됩니다.) -> root가 /content일 경우만\n",
        "\n",
        "%%writefile /content/trl/examples/research_projects/stack_llama_2/scripts/sft_llama2.py\n",
        "# Fine-Tune Llama2-7b on SE paired dataset\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    model_name: Optional[str] = field(default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": \"the model name\"})\n",
        "    log_with: Optional[str] = field(default=\"wandb\", metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
        "\n",
        "    dataset_name: Optional[str] = field(default=\"lvwerra/stack-exchange-paired\", metadata={\"help\": \"the dataset name\"})\n",
        "    subset: Optional[str] = field(default=\"data/finetune\", metadata={\"help\": \"the subset to use\"})\n",
        "    split: Optional[str] = field(default=\"train\", metadata={\"help\": \"the split to use\"})\n",
        "    size_valid_set: Optional[int] = field(default=4000, metadata={\"help\": \"the size of the validation set\"})\n",
        "    streaming: Optional[bool] = field(default=True, metadata={\"help\": \"whether to stream the dataset\"})\n",
        "    shuffle_buffer: Optional[int] = field(default=5000, metadata={\"help\": \"the shuffle buffer size\"})\n",
        "    seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"the sequence length\"})\n",
        "    num_workers: Optional[int] = field(default=4, metadata={\"help\": \"the number of workers\"})\n",
        "\n",
        "    max_steps: Optional[int] = field(default=500, metadata={\"help\": \"the maximum number of sgd steps\"})\n",
        "    logging_steps: Optional[int] = field(default=10, metadata={\"help\": \"the logging frequency\"})\n",
        "    save_steps: Optional[int] = field(default=10, metadata={\"help\": \"the saving frequency\"})\n",
        "    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"the per device train batch size\"})\n",
        "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the per device eval batch size\"})\n",
        "    gradient_accumulation_steps: Optional[int] = field(default=2, metadata={\"help\": \"the gradient accumulation steps\"})\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n",
        "    )\n",
        "    group_by_length: Optional[bool] = field(default=False, metadata={\"help\": \"whether to group by length\"})\n",
        "    packing: Optional[bool] = field(default=True, metadata={\"help\": \"whether to use packing for SFTTrainer\"})\n",
        "\n",
        "    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n",
        "    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n",
        "    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n",
        "\n",
        "    learning_rate: Optional[float] = field(default=1e-4, metadata={\"help\": \"the learning rate\"})\n",
        "    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n",
        "    num_warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"the number of warmup steps\"})\n",
        "    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n",
        "    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n",
        "\n",
        "    output_dir: Optional[str] = field(default=\"./results\", metadata={\"help\": \"the output directory\"})\n",
        "    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n",
        "\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args = parser.parse_args_into_dataclasses()[0]\n",
        "\n",
        "if script_args.group_by_length and script_args.packing:\n",
        "    raise ValueError(\"Cannot use both packing and group by length\")\n",
        "\n",
        "\n",
        "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
        "    \"\"\"\n",
        "    Estimate the average number of characters per token in the dataset.\n",
        "    \"\"\"\n",
        "    total_characters, total_tokens = 0, 0\n",
        "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
        "        text = prepare_sample_text(example)\n",
        "        total_characters += len(text)\n",
        "        if tokenizer.is_fast:\n",
        "            total_tokens += len(tokenizer(text).tokens())\n",
        "        else:\n",
        "            total_tokens += len(tokenizer.tokenize(text))\n",
        "\n",
        "    return total_characters / total_tokens\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['chosen']}\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def create_datasets(tokenizer, args):\n",
        "    dataset = load_dataset(\n",
        "        args.dataset_name,\n",
        "#        data_dir=args.subset,\n",
        "        split=args.split,\n",
        "        use_auth_token=True,\n",
        "        num_proc=args.num_workers if not args.streaming else None,\n",
        "        streaming=args.streaming,\n",
        "    )\n",
        "\n",
        "    if args.streaming:\n",
        "        print(\"Loading the dataset in streaming mode\")\n",
        "        valid_data = dataset.take(args.size_valid_set)\n",
        "        train_data = dataset.skip(args.size_valid_set)\n",
        "        train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n",
        "    else:\n",
        "        dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
        "        train_data = dataset[\"train\"]\n",
        "        valid_data = dataset[\"test\"]\n",
        "        print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n",
        "\n",
        "    chars_per_token = chars_token_ratio(train_data, tokenizer)\n",
        "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
        "\n",
        "    train_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        train_data,\n",
        "        formatting_func=prepare_sample_text,\n",
        "        infinite=True,\n",
        "        seq_length=args.seq_length,\n",
        "        chars_per_token=chars_per_token,\n",
        "    )\n",
        "    valid_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        valid_data,\n",
        "        formatting_func=prepare_sample_text,\n",
        "        infinite=False,\n",
        "        seq_length=args.seq_length,\n",
        "        chars_per_token=chars_per_token,\n",
        "    )\n",
        "    return train_dataset, valid_dataset\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    script_args.model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": Accelerator().local_process_index},\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True,\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=script_args.lora_r,\n",
        "    lora_alpha=script_args.lora_alpha,\n",
        "    lora_dropout=script_args.lora_dropout,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=script_args.output_dir,\n",
        "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
        "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
        "    learning_rate=script_args.learning_rate,\n",
        "    logging_steps=script_args.logging_steps,\n",
        "    max_steps=script_args.max_steps,\n",
        "    report_to=script_args.log_with,\n",
        "    save_steps=script_args.save_steps,\n",
        "    group_by_length=script_args.group_by_length,\n",
        "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
        "    warmup_steps=script_args.num_warmup_steps,\n",
        "    optim=script_args.optimizer_type,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    run_name=\"sft_llama2\",\n",
        ")\n",
        "\n",
        "train_dataset, eval_dataset = create_datasets(tokenizer, script_args)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    packing=script_args.packing,\n",
        "    max_seq_length=None,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(script_args.output_dir)\n",
        "\n",
        "output_dir = os.path.join(script_args.output_dir, \"final_checkpoint\")\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "# Free memory for merging weights\n",
        "del base_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
        "model.save_pretrained(output_merged_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "VzHEg6qk5GVT",
        "outputId": "8307e146-6a2f-442b-8000-5e2826a59505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/trl/examples/research_projects/stack_llama_2/scripts/sft_llama2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "cAGubdym_-Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### sft 실행\n",
        "\n",
        "# @markdown model 이름, dataset 이름, output_dir 이름 입력\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown 아래 입력 기본값은 제가 실험한것:\n",
        "model_name = \"EleutherAI/gpt-j-6b\" # @param {type:\"string\"}\n",
        "dataset_name = \"banghua/tldr_reward_model_labeled\" # @param {type:\"string\"}\n",
        "output_dir = \"sft\" # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "\n",
        "\n",
        "!accelerate launch trl/examples/research_projects/stack_llama_2/scripts/sft_llama2.py --model_name={model_name} --dataset_name={dataset_name} --output_dir={output_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRz5PskV866v",
        "outputId": "1f4e00fe-d36f-4da2-b372-e276728bb4b9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-12 12:53:10.685905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-09-12 12:53:16.956031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=True' instead.\n",
            "  warnings.warn(\n",
            "Loading the dataset in streaming mode\n",
            "100% 400/400 [00:04<00:00, 82.94it/s] \n",
            "The character to token ratio of the dataset is: 4.13\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:465: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictionary returned by each element of the dataset. Make sure you know what you are doing.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:227: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230912_130029-bhccn0xy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msft_llama2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/choi_jh/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/choi_jh/huggingface/runs/bhccn0xy\u001b[0m\n",
            "  0% 0/500 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 2.5502, 'learning_rate': 1e-05, 'epoch': 0.02}\n",
            "{'loss': 2.517, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
            "{'loss': 2.5347, 'learning_rate': 3e-05, 'epoch': 0.06}\n",
            "{'loss': 2.4758, 'learning_rate': 4e-05, 'epoch': 0.08}\n",
            "{'loss': 2.4366, 'learning_rate': 5e-05, 'epoch': 0.1}\n",
            "{'loss': 2.3691, 'learning_rate': 6e-05, 'epoch': 0.12}\n",
            "{'loss': 2.3451, 'learning_rate': 7e-05, 'epoch': 0.14}\n",
            "{'loss': 2.3233, 'learning_rate': 8e-05, 'epoch': 0.16}\n",
            "{'loss': 2.3148, 'learning_rate': 9e-05, 'epoch': 0.18}\n",
            "{'loss': 2.3175, 'learning_rate': 0.0001, 'epoch': 0.2}\n",
            "{'loss': 2.2674, 'learning_rate': 9.98458666866564e-05, 'epoch': 0.22}\n",
            "{'loss': 2.2511, 'learning_rate': 9.938441702975689e-05, 'epoch': 0.24}\n",
            "{'loss': 2.2436, 'learning_rate': 9.861849601988383e-05, 'epoch': 0.26}\n",
            "{'loss': 2.24, 'learning_rate': 9.755282581475769e-05, 'epoch': 0.28}\n",
            "{'loss': 2.2249, 'learning_rate': 9.619397662556435e-05, 'epoch': 0.3}\n",
            "{'loss': 2.2465, 'learning_rate': 9.45503262094184e-05, 'epoch': 0.32}\n",
            "{'loss': 2.2254, 'learning_rate': 9.263200821770461e-05, 'epoch': 0.34}\n",
            "{'loss': 2.1746, 'learning_rate': 9.045084971874738e-05, 'epoch': 0.36}\n",
            "{'loss': 2.2088, 'learning_rate': 8.802029828000156e-05, 'epoch': 0.38}\n",
            "{'loss': 2.2082, 'learning_rate': 8.535533905932738e-05, 'epoch': 0.4}\n",
            "{'loss': 2.189, 'learning_rate': 8.247240241650918e-05, 'epoch': 0.42}\n",
            "{'loss': 2.2111, 'learning_rate': 7.938926261462366e-05, 'epoch': 0.44}\n",
            "{'loss': 2.1601, 'learning_rate': 7.612492823579745e-05, 'epoch': 0.46}\n",
            "{'loss': 2.1625, 'learning_rate': 7.269952498697734e-05, 'epoch': 0.48}\n",
            "{'loss': 2.1939, 'learning_rate': 6.91341716182545e-05, 'epoch': 0.5}\n",
            "{'loss': 2.1995, 'learning_rate': 6.545084971874738e-05, 'epoch': 0.52}\n",
            "{'loss': 2.2274, 'learning_rate': 6.167226819279528e-05, 'epoch': 0.54}\n",
            "{'loss': 2.1784, 'learning_rate': 5.782172325201155e-05, 'epoch': 0.56}\n",
            "{'loss': 2.21, 'learning_rate': 5.392295478639225e-05, 'epoch': 0.58}\n",
            "{'loss': 2.1709, 'learning_rate': 5e-05, 'epoch': 0.6}\n",
            "{'loss': 2.1712, 'learning_rate': 4.607704521360776e-05, 'epoch': 0.62}\n",
            "{'loss': 2.2012, 'learning_rate': 4.2178276747988446e-05, 'epoch': 0.64}\n",
            "{'loss': 2.2094, 'learning_rate': 3.832773180720475e-05, 'epoch': 0.66}\n",
            "{'loss': 2.1085, 'learning_rate': 3.4549150281252636e-05, 'epoch': 0.68}\n",
            "{'loss': 2.1429, 'learning_rate': 3.086582838174551e-05, 'epoch': 0.7}\n",
            "{'loss': 2.1284, 'learning_rate': 2.7300475013022663e-05, 'epoch': 0.72}\n",
            "{'loss': 2.1326, 'learning_rate': 2.3875071764202563e-05, 'epoch': 0.74}\n",
            "{'loss': 2.0989, 'learning_rate': 2.061073738537635e-05, 'epoch': 0.76}\n",
            "{'loss': 2.1719, 'learning_rate': 1.7527597583490822e-05, 'epoch': 0.78}\n",
            "{'loss': 2.1906, 'learning_rate': 1.4644660940672627e-05, 'epoch': 0.8}\n",
            "{'loss': 2.204, 'learning_rate': 1.1979701719998453e-05, 'epoch': 0.82}\n",
            "{'loss': 2.2656, 'learning_rate': 9.549150281252633e-06, 'epoch': 0.84}\n",
            "{'loss': 2.1931, 'learning_rate': 7.367991782295391e-06, 'epoch': 0.86}\n",
            "{'loss': 2.1717, 'learning_rate': 5.449673790581611e-06, 'epoch': 0.88}\n",
            "{'loss': 2.2432, 'learning_rate': 3.8060233744356633e-06, 'epoch': 0.9}\n",
            "{'loss': 2.2088, 'learning_rate': 2.4471741852423237e-06, 'epoch': 0.92}\n",
            "{'loss': 2.2056, 'learning_rate': 1.3815039801161721e-06, 'epoch': 0.94}\n",
            "{'loss': 2.1877, 'learning_rate': 6.15582970243117e-07, 'epoch': 0.96}\n",
            "{'loss': 2.2061, 'learning_rate': 1.5413331334360182e-07, 'epoch': 0.98}\n",
            "{'loss': 2.1872, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "{'train_runtime': 3658.9267, 'train_samples_per_second': 1.093, 'train_steps_per_second': 0.137, 'train_loss': 2.240117992401123, 'epoch': 1.0}\n",
            "100% 500/500 [59:05<00:00,  7.09s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▂▂▃▄▅▆▇▇█████▇▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▇█▇▅▅▄▄▄▃▃▃▃▃▂▃▂▃▂▂▃▃▂▃▂▃▃▁▁▂▁▂▃▄▂▂▃▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 500\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 2.1872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 7.4454026944512e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 2.24012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 3658.9267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 1.093\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msft_llama2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/choi_jh/huggingface/runs/bhccn0xy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230912_130029-bhccn0xy/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "fsd33WIB-BIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DPO 실행전 수정\n",
        "\n",
        "- 데이터셋 내용 관련 수정 (데이터셋 이름 변경, tokenizer 이름 변경)\n",
        "- v100 기준 b16 -> fp16"
      ],
      "metadata": {
        "id": "pO8G75nR-CnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 전처리 EOS 때문에 DPO 코드에서 에러나서 변경"
      ],
      "metadata": {
        "id": "HXojPeMWgfoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"banghua/tldr_reward_model_labeled\",\n",
        "    split=\"train\"\n",
        ")\n",
        "dataset = dataset.map(lambda c : {\"chosen\": c['chosen'].replace('<|endoftext|>', '')})\n",
        "dataset = dataset.map(lambda c : {\"rejected\": c['rejected'].replace('<|endoftext|>', '')})\n",
        "dataset.save_to_disk('data')"
      ],
      "metadata": {
        "id": "5ljn6BPKgfZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "DrquoP40W62P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### DPO TL;DR 데이터 맞춤 (그냥 이부분 실행하면 됩니다.) -> root가 /content일 경우만\n",
        "\n",
        "%%writefile /content/trl/examples/research_projects/stack_llama_2/scripts/dpo_llama2.py\n",
        "# 0. imports\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset, load_from_disk\n",
        "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
        "from transformers import AutoTokenizer, HfArgumentParser, TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "\n",
        "\n",
        "# Define and parse arguments.\n",
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    \"\"\"\n",
        "    The arguments for the DPO training script.\n",
        "    \"\"\"\n",
        "\n",
        "    # data parameters\n",
        "    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n",
        "\n",
        "    # training parameters\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=\"../sft/results/final_checkpoint\",\n",
        "        metadata={\"help\": \"the location of the SFT model name or path\"},\n",
        "    )\n",
        "    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"optimizer learning rate\"})\n",
        "    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n",
        "    warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"the number of warmup steps\"})\n",
        "    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n",
        "    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n",
        "\n",
        "    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"train batch size per device\"})\n",
        "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"eval batch size per device\"})\n",
        "    gradient_accumulation_steps: Optional[int] = field(\n",
        "        default=4, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
        "    )\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n",
        "    )\n",
        "\n",
        "    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n",
        "    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n",
        "    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n",
        "\n",
        "    max_prompt_length: Optional[int] = field(default=512, metadata={\"help\": \"the maximum prompt length\"})\n",
        "    max_length: Optional[int] = field(default=1024, metadata={\"help\": \"the maximum sequence length\"})\n",
        "    max_steps: Optional[int] = field(default=1000, metadata={\"help\": \"max number of training steps\"})\n",
        "    logging_steps: Optional[int] = field(default=10, metadata={\"help\": \"the logging frequency\"})\n",
        "    save_steps: Optional[int] = field(default=100, metadata={\"help\": \"the saving frequency\"})\n",
        "    eval_steps: Optional[int] = field(default=100, metadata={\"help\": \"the evaluation frequency\"})\n",
        "\n",
        "    output_dir: Optional[str] = field(default=\"./results\", metadata={\"help\": \"the output directory\"})\n",
        "    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n",
        "\n",
        "    # instrumentation\n",
        "    sanity_check: Optional[bool] = field(default=False, metadata={\"help\": \"only train on 1000 samples\"})\n",
        "    report_to: Optional[str] = field(\n",
        "        default=\"wandb\",\n",
        "        metadata={\n",
        "            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n",
        "            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n",
        "            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n",
        "        },\n",
        "    )\n",
        "    # debug argument for distributed training\n",
        "    ignore_bias_buffers: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n",
        "            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "def get_stack_exchange_paired(\n",
        "    data_dir: str = \"data/rl\",\n",
        "    sanity_check: bool = False,\n",
        "    cache_dir: str = None,\n",
        "    num_proc=24,\n",
        ") -> Dataset:\n",
        "    \"\"\"Load the stack-exchange-paired dataset from Hugging Face and convert it to the necessary format.\n",
        "\n",
        "    The dataset is converted to a dictionary with the following structure:\n",
        "    {\n",
        "        'prompt': List[str],\n",
        "        'chosen': List[str],\n",
        "        'rejected': List[str],\n",
        "    }\n",
        "\n",
        "    Prompts are structured as follows:\n",
        "      \"Question: \" + <prompt> + \"\\n\\nAnswer: \"\n",
        "    \"\"\"\n",
        "    dataset = load_from_disk(\n",
        "        \"data\",\n",
        "  #      split=\"train\",\n",
        "  #      cache_dir=cache_dir,\n",
        "  #      data_dir=data_dir,\n",
        "    )\n",
        "    original_columns = dataset.column_names\n",
        "\n",
        "    if sanity_check:\n",
        "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
        "\n",
        "    def return_prompt_and_responses(samples) -> Dict[str, str]:\n",
        "        return {\n",
        "            \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"prompt\"]],\n",
        "            \"chosen\": samples[\"chosen\"],\n",
        "            \"rejected\": samples[\"rejected\"],\n",
        "        }\n",
        "\n",
        "    return dataset.map(\n",
        "        return_prompt_and_responses,\n",
        "        batched=True,\n",
        "        num_proc=num_proc,\n",
        "        remove_columns=original_columns,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = HfArgumentParser(ScriptArguments)\n",
        "    script_args = parser.parse_args_into_dataclasses()[0]\n",
        "\n",
        "    # 1. load a pretrained model\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        script_args.model_name_or_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    if script_args.ignore_bias_buffers:\n",
        "        # torch distributed hack\n",
        "        model._ddp_params_and_buffers_to_ignore = [\n",
        "            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
        "        ]\n",
        "\n",
        "    model_ref = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        script_args.model_name_or_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\", add_eos_token=True)\n",
        "    tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "    # 2. Load the Stack-exchange paired dataset\n",
        "    train_dataset = get_stack_exchange_paired(data_dir=\"data/rl\", sanity_check=script_args.sanity_check)\n",
        "    train_dataset = train_dataset.filter(\n",
        "        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= script_args.max_length\n",
        "        and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= script_args.max_length\n",
        "    )\n",
        "\n",
        "    # 3. Load evaluation dataset\n",
        "    eval_dataset = get_stack_exchange_paired(data_dir=\"data/evaluation\", sanity_check=True)\n",
        "    eval_dataset = eval_dataset.filter(\n",
        "        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= script_args.max_length\n",
        "        and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= script_args.max_length\n",
        "    )\n",
        "\n",
        "    # 4. initialize training arguments:\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
        "        max_steps=script_args.max_steps,\n",
        "        logging_steps=script_args.logging_steps,\n",
        "        save_steps=script_args.save_steps,\n",
        "        gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
        "        gradient_checkpointing=script_args.gradient_checkpointing,\n",
        "        learning_rate=script_args.learning_rate,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=script_args.eval_steps,\n",
        "        output_dir=script_args.output_dir,\n",
        "        report_to=script_args.report_to,\n",
        "        lr_scheduler_type=script_args.lr_scheduler_type,\n",
        "        warmup_steps=script_args.warmup_steps,\n",
        "        optim=script_args.optimizer_type,\n",
        "        bf16=False,\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        run_name=\"dpo_gptj\",\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=script_args.lora_r,\n",
        "        lora_alpha=script_args.lora_alpha,\n",
        "        lora_dropout=script_args.lora_dropout,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"v_proj\",\n",
        "            \"k_proj\",\n",
        "            \"out_proj\",\n",
        "            \"fc_in\",\n",
        "            \"fc_out\",\n",
        "            \"wte\",\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # 5. initialize the DPO trainer\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model,\n",
        "        model_ref,\n",
        "        args=training_args,\n",
        "        beta=script_args.beta,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        peft_config=peft_config,\n",
        "        max_prompt_length=script_args.max_prompt_length,\n",
        "        max_length=script_args.max_length,\n",
        "    )\n",
        "\n",
        "    # 6. train\n",
        "    dpo_trainer.train()\n",
        "    dpo_trainer.save_model(script_args.output_dir)\n",
        "\n",
        "    # 7. save\n",
        "    output_dir = os.path.join(script_args.output_dir, \"final_checkpoint\")\n",
        "    dpo_trainer.model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wxf9cs5D-B2N",
        "outputId": "94fca445-6dc4-4da1-d59b-35804cccbe2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/trl/examples/research_projects/stack_llama_2/scripts/dpo_llama2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "0aLMSa6sW9xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### DPO 실행\n",
        "\n",
        "# @markdown model_name_or_path 이름, output_dir 이름 입력\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown 아래 입력 기본값은 제가 실험한것:\n",
        "model_name_or_path = \"sft/final_checkpoint\" # @param {type:\"string\"}\n",
        "output_dir = \"dpo\" # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "\n",
        "\n",
        "!accelerate launch /content/trl/examples/research_projects/stack_llama_2/scripts/dpo_llama2.py --model_name_or_path={model_name_or_path} --output_dir={output_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEIYDP0UAF8j",
        "outputId": "ce372d32-35e4-4c4f-d9c0-7c68c27a0bf1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-12 14:06:51.053842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-09-12 14:06:57.624762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/42.5M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  10% 4.19M/42.5M [00:03<00:28, 1.35MB/s]\u001b[A\n",
            "Downloading data:  30% 12.6M/42.5M [00:04<00:09, 3.29MB/s]\u001b[A\n",
            "Downloading data:  49% 21.0M/42.5M [00:07<00:06, 3.21MB/s]\u001b[A\n",
            "Downloading data:  69% 29.4M/42.5M [00:08<00:03, 4.29MB/s]\u001b[A\n",
            "Downloading data: 100% 42.5M/42.5M [00:09<00:00, 4.69MB/s]\n",
            "Downloading data files: 100% 1/1 [00:09<00:00,  9.06s/it]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1581.56it/s]\n",
            "Generating train split: 100% 176163/176163 [00:00<00:00, 235467.33 examples/s]\n",
            "Map (num_proc=24): 100% 176163/176163 [00:00<00:00, 211481.30 examples/s]\n",
            "Filter: 100% 176163/176163 [00:00<00:00, 180436.64 examples/s]\n",
            "Map (num_proc=24): 100% 1000/1000 [00:00<00:00, 2971.42 examples/s]\n",
            "Filter: 100% 1000/1000 [00:00<00:00, 97200.62 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcjhyeok\u001b[0m (\u001b[33mchoi_jh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230912_141852-466rq6b4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_gptj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/choi_jh/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/choi_jh/huggingface/runs/466rq6b4\u001b[0m\n",
            "  0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "{'loss': 0.8037, 'learning_rate': 4.4999999999999996e-05, 'rewards/chosen': -2.98219895362854, 'rewards/rejected': -2.869689464569092, 'rewards/accuracies': 0.41874998807907104, 'rewards/margins': -0.11250938475131989, 'logps/rejected': -87.01798248291016, 'logps/chosen': -89.39556121826172, 'logits/rejected': 0.026925548911094666, 'logits/chosen': 0.02893729880452156, 'epoch': 0.01}\n",
            "{'loss': 0.7268, 'learning_rate': 9.5e-05, 'rewards/chosen': -2.661783456802368, 'rewards/rejected': -2.6852259635925293, 'rewards/accuracies': 0.5062500238418579, 'rewards/margins': 0.023442450910806656, 'logps/rejected': -84.1792221069336, 'logps/chosen': -83.78901672363281, 'logits/rejected': 0.03468726575374603, 'logits/chosen': 0.04219017177820206, 'epoch': 0.02}\n",
            "{'loss': 0.7911, 'learning_rate': 0.000145, 'rewards/chosen': -2.2598366737365723, 'rewards/rejected': -2.150993824005127, 'rewards/accuracies': 0.46875, 'rewards/margins': -0.10884282737970352, 'logps/rejected': -81.36605072021484, 'logps/chosen': -81.07575225830078, 'logits/rejected': 0.051357071846723557, 'logits/chosen': 0.049581751227378845, 'epoch': 0.02}\n",
            "  3% 31/1000 [06:01<3:07:57, 11.64s/it]Traceback (most recent call last):\n",
            "  File \"/content/trl/examples/research_projects/stack_llama_2/scripts/dpo_llama2.py\", line 219, in <module>\n",
            "    dpo_trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1553, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1835, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2679, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py\", line 393, in compute_loss\n",
            "    loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"train\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py\", line 360, in get_batch_metrics\n",
            "    ) = self.concatenated_forward(self.ref_model, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py\", line 314, in concatenated_forward\n",
            "    all_logits = model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 632, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 620, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 918, in forward\n",
            "    return self.base_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 94, in forward\n",
            "    return self.model.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\", line 855, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\", line 690, in forward\n",
            "    outputs = block(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\", line 321, in forward\n",
            "    feed_forward_hidden_states = self.mlp(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\", line 284, in forward\n",
            "    hidden_states = self.fc_out(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 248, in forward\n",
            "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\", line 579, in matmul_4bit\n",
            "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
            "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, state).to(A.dtype).t(), bias)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.77 GiB total capacity; 13.36 GiB already allocated; 174.12 MiB free; 14.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch ▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/logits/chosen ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/logits/rejected ▁▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/logps/chosen ▁▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/logps/rejected ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss █▁▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/rewards/accuracies ▁█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/rewards/chosen ▁▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/rewards/margins ▁█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/rewards/rejected ▁▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 0.02\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.00015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/logits/chosen 0.04958\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/logits/rejected 0.05136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/logps/chosen -81.07575\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/logps/rejected -81.36605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0.7911\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/rewards/accuracies 0.46875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/rewards/chosen -2.25984\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/rewards/margins -0.10884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/rewards/rejected -2.15099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdpo_gptj\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/choi_jh/huggingface/runs/466rq6b4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230912_141852-466rq6b4/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 986, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 628, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/trl/examples/research_projects/stack_llama_2/scripts/dpo_llama2.py', '--model_name_or_path=sft/final_checkpoint', '--output_dir=dpo']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "5YROYudEXpV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### colab은 cuda 메모리 부족으로 다른곳으로 테스트\n",
        "- gpu 3090 기준으로 수행\n",
        "- 코드는 colab과 동일\n",
        "\n",
        "\n",
        "- sft\n",
        "https://wandb.ai/choi_jh/huggingface/runs/0jpc2z0l\n",
        "- dpo\n",
        "https://wandb.ai/choi_jh/huggingface/runs/shi0hgc5\n",
        "\n",
        "> tldr 데이터셋을 쓴것은 맞지만 따로 후처리를 했을거 같습니다.\n",
        "(DPO 학습시 요약데이터에서 선택, 거절 과같은 2개의 보기가 존재해야하는데 기존 tldr 데이터셋에는 없음)"
      ],
      "metadata": {
        "id": "MNtQeJKUW5Z9"
      }
    }
  ]
}